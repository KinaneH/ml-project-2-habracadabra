{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from src.cfg import *\n",
    "from src.data_cleaning.data_loader import *\n",
    "from src.utils import preprocess_train_data, preprocess_test_data, get_model, evaluate_model\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "pos_tweets = load_data(TRAIN_POS_PATH)\n",
    "neg_tweets = load_data(TRAIN_NEG_PATH)\n",
    "\n",
    "# Just select subset to see if it works\n",
    "pos_tweets = pos_tweets.head(20)\n",
    "neg_tweets = neg_tweets.head(20)\n",
    "\n",
    "# Load full data\n",
    "#pos_tweets = load_data(TRAIN_POS_FULL_PATH)\n",
    "#neg_tweets = load_data(TRAIN_NEG_FULL_PATH)\n",
    "\n",
    "X_train, X_val, y_train, y_val = preprocess_train_data(pos_tweets, neg_tweets, show_lengths=True, show_samples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess test data\n",
    "test_ids, cleaned_test_texts = preprocess_test_data(TEST_PATH)\n",
    "print(f\"Processed {len(test_ids)} test samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====== NAIVE BAYES ======\n",
    "SELECTED_MODEL = 'naive_bayes'\n",
    "\n",
    "# Ensure the \"submissions_classifiers\" folder exists\n",
    "submissions_folder = \"submissions_classifiers\"\n",
    "os.makedirs(submissions_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Train, evaluate, and save submission for the model\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Running GridSearchCV for {SELECTED_MODEL}...\")\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', get_model(SELECTED_MODEL))\n",
    "])\n",
    "\n",
    "param_grid = PARAM_GRID[SELECTED_MODEL]\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Best parameters for {SELECTED_MODEL}: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_f1 = evaluate_model(best_model, X_val, y_val, metric='f1')\n",
    "print(f\"Validation F1 score for {SELECTED_MODEL}: {val_f1:.4f}\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "# Transform test data using the vectorizer from the best model\n",
    "vectorizer = best_model.named_steps['vectorizer']  # Extract vectorizer from pipeline\n",
    "X_test_tfidf = vectorizer.transform(cleaned_test_texts)\n",
    "\n",
    "# Make predictions\n",
    "classifier = best_model.named_steps['classifier']  # Extract classifier from pipeline\n",
    "test_predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"Prediction\": test_predictions\n",
    "})\n",
    "\n",
    "submission_file_path = os.path.join(submissions_folder, f\"submission_{SELECTED_MODEL}.csv\")\n",
    "submission.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Submission file for {SELECTED_MODEL} saved to {submission_file_path}\")\n",
    "print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====== LOGISTIC REGRESSION ======\n",
    "SELECTED_MODEL = 'logistic_regression'\n",
    "\n",
    "# Ensure the \"submissions_classifiers\" folder exists\n",
    "submissions_folder = \"submissions_classifiers\"\n",
    "os.makedirs(submissions_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Train, evaluate, and save submission for the model\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Running GridSearchCV for {SELECTED_MODEL}...\")\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', get_model(SELECTED_MODEL))\n",
    "])\n",
    "\n",
    "param_grid = PARAM_GRID[SELECTED_MODEL]\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Best parameters for {SELECTED_MODEL}: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_f1 = evaluate_model(best_model, X_val, y_val, metric='f1')\n",
    "print(f\"Validation F1 score for {SELECTED_MODEL}: {val_f1:.4f}\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "# Transform test data using the vectorizer from the best model\n",
    "vectorizer = best_model.named_steps['vectorizer']  # Extract vectorizer from pipeline\n",
    "X_test_tfidf = vectorizer.transform(cleaned_test_texts)\n",
    "\n",
    "# Make predictions\n",
    "classifier = best_model.named_steps['classifier']  # Extract classifier from pipeline\n",
    "test_predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"Prediction\": test_predictions\n",
    "})\n",
    "\n",
    "submission_file_path = os.path.join(submissions_folder, f\"submission_{SELECTED_MODEL}.csv\")\n",
    "submission.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Submission file for {SELECTED_MODEL} saved to {submission_file_path}\")\n",
    "print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ====== SUPPORT VECTOR MACHINES ======\n",
    "SELECTED_MODEL = 'svm'\n",
    "\n",
    "# Ensure the \"submissions_classifiers\" folder exists\n",
    "submissions_folder = \"submissions_classifiers\"\n",
    "os.makedirs(submissions_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Train, evaluate, and save submission for the model\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Running GridSearchCV for {SELECTED_MODEL}...\")\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('classifier', get_model(SELECTED_MODEL))\n",
    "])\n",
    "\n",
    "param_grid = PARAM_GRID[SELECTED_MODEL]\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Best parameters for {SELECTED_MODEL}: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_f1 = evaluate_model(best_model, X_val, y_val, metric='f1')\n",
    "print(f\"Validation F1 score for {SELECTED_MODEL}: {val_f1:.4f}\")\n",
    "print(\"-----------------------------------------------------------\\n\")\n",
    "\n",
    "# Transform test data using the vectorizer from the best model\n",
    "vectorizer = best_model.named_steps['vectorizer']  # Extract vectorizer from pipeline\n",
    "X_test_tfidf = vectorizer.transform(cleaned_test_texts)\n",
    "\n",
    "# Make predictions\n",
    "classifier = best_model.named_steps['classifier']  # Extract classifier from pipeline\n",
    "test_predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"Id\": test_ids,\n",
    "    \"Prediction\": test_predictions\n",
    "})\n",
    "\n",
    "submission_file_path = os.path.join(submissions_folder, f\"submission_{SELECTED_MODEL}.csv\")\n",
    "submission.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(\"-----------------------------------------------------------\")\n",
    "print(f\"Submission file for {SELECTED_MODEL} saved to {submission_file_path}\")\n",
    "print(\"-----------------------------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
